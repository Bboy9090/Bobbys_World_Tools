name: üîç Audit Plan
description: Plan a code audit to find placeholders, mocks, or technical debt
title: "[Audit]: "
labels: ["audit", "technical-debt"]
body:
  - type: markdown
    attributes:
      value: |
        ## Audit Plan ‚Äî Truth-First Methodology
        
        This template helps plan systematic code audits to ensure production readiness.
        
        **Prime Rule:** Audit with evidence. Document findings. Fix or track.

  - type: dropdown
    id: audit_type
    attributes:
      label: Audit Type
      description: What kind of audit is this?
      options:
        - "Placeholder/Mock Detection"
        - "Security Audit"
        - "Performance Audit"
        - "Code Quality Audit"
        - "Documentation Audit"
        - "Test Coverage Audit"
        - "Dependency Audit"
        - "Other"
    validations:
      required: true

  - type: textarea
    id: scope
    attributes:
      label: Audit Scope
      description: What will be audited?
      placeholder: |
        - All TypeScript files in `src/api/`
        - All Rust files in `crates/bootforge-usb/`
        - All scripts in `scripts/`
      value: |
        - 
        - 
        - 
    validations:
      required: true

  - type: textarea
    id: objective
    attributes:
      label: Audit Objective
      description: What are you looking for? What questions will this audit answer?
      placeholder: |
        Find all instances of:
        - Mock data in production code paths
        - TODO/FIXME comments in runtime code
        - Fake success returns (`return { success: true }` without actual work)
        - Placeholder implementations
    validations:
      required: true

  - type: textarea
    id: criteria
    attributes:
      label: Audit Criteria
      description: What makes something pass/fail the audit?
      placeholder: |
        **Pass:**
        - Real implementations with actual business logic
        - Feature flags for incomplete features (OFF by default)
        - Clear error messages for unavailable features
        
        **Fail:**
        - Mock data in production paths
        - `// TODO: implement this` in runtime code
        - Fake success returns
        - Silent failures
    validations:
      required: true

  - type: textarea
    id: methodology
    attributes:
      label: Audit Methodology
      description: How will you conduct the audit?
      placeholder: |
        1. Automated scan with ripgrep for keywords:
           - TODO, FIXME, HACK, STUB, MOCK
           - "coming soon", "not implemented"
           - "return { success: true }"
        
        2. Manual review of flagged files:
           - Verify if actually problematic
           - Classify severity (Critical/High/Medium/Low)
        
        3. Document findings in audit report:
           - File location
           - Issue description
           - Severity
           - Proposed fix
        
        4. Create tracking issues for each finding
        
        5. Fix Critical/High severity issues immediately
      value: |
        1. 
        2. 
        3. 
    validations:
      required: true

  - type: input
    id: assignee
    attributes:
      label: Audit Lead
      description: Who will conduct this audit?
      placeholder: "@username or agent name (e.g., Audit Hunter Agent)"

  - type: input
    id: deadline
    attributes:
      label: Target Completion Date
      description: When should this audit be complete?
      placeholder: "2025-12-31"

  - type: textarea
    id: tools
    attributes:
      label: Tools & Scripts
      description: What tools or scripts will be used?
      placeholder: |
        - `ripgrep` for keyword search
        - `eslint` for code quality
        - `cargo clippy` for Rust
        - Custom script: `scripts/find-placeholders.js`

  - type: textarea
    id: expected_findings
    attributes:
      label: Expected Findings (Hypothesis)
      description: What do you expect to find? (Can be wrong - that's why we audit!)
      placeholder: |
        Based on recent development:
        - ~10-15 TODO comments in API code
        - ~5 mock returns in device scanning
        - ~3 placeholder error messages

  - type: textarea
    id: output_format
    attributes:
      label: Output Format
      description: How will findings be reported?
      placeholder: |
        Create `docs/audits/placeholder-audit-2025-12.md` with:
        
        ## Summary
        - Total files scanned: X
        - Issues found: Y
        - Critical: Z
        - High: A
        - Medium: B
        - Low: C
        
        ## Findings
        
        ### Critical: [Category]
        **File:** `src/api/devices.ts:45`
        **Issue:** Mock device list returned in production
        **Code:**
        ```typescript
        return { devices: MOCK_DEVICES };
        ```
        **Fix:** Implement real device scanning
        **Tracking:** Issue #XXX
      value: |
        Create `docs/audits/[audit-name]-[date].md`

  - type: checkboxes
    id: deliverables
    attributes:
      label: Audit Deliverables
      description: What will be produced?
      options:
        - label: Audit report document
        - label: List of findings with severity
        - label: Tracking issues for each finding
        - label: PRs to fix Critical/High severity issues
        - label: Plan to fix Medium/Low severity issues

  - type: dropdown
    id: urgency
    attributes:
      label: Audit Urgency
      description: How urgent is this audit?
      options:
        - "Critical - Blocking production release"
        - "High - Needed soon"
        - "Medium - Planned maintenance"
        - "Low - Nice to have"
    validations:
      required: true

  - type: textarea
    id: success_criteria
    attributes:
      label: Success Criteria
      description: How will you know the audit was successful?
      placeholder: |
        - [ ] All files in scope scanned
        - [ ] All findings documented
        - [ ] Critical issues fixed
        - [ ] High issues have tracking issues
        - [ ] Audit report reviewed by team
        - [ ] Follow-up plan created for medium/low issues

  - type: textarea
    id: follow_up
    attributes:
      label: Follow-Up Plan
      description: What happens after the audit?
      placeholder: |
        - Fix Critical issues immediately (this sprint)
        - Create PRs for High issues (next sprint)
        - Track Medium/Low issues in backlog
        - Schedule re-audit in 3 months
        - Add CI checks to prevent regressions

  - type: textarea
    id: related_audits
    attributes:
      label: Related Audits
      description: Link to previous or related audits
      placeholder: |
        - Previous placeholder audit: docs/audits/placeholder-audit-2025-09.md
        - Related security audit: #456

  - type: textarea
    id: additional
    attributes:
      label: Additional Context
      description: Any other relevant information
      placeholder: "This audit was requested by stakeholders before production launch."
